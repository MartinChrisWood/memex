[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "And so I’ve stolen the example/template post. Anyway, intending to use this to record bits and pieces of what I learn, mostly for my sake."
  },
  {
    "objectID": "posts/review_DSATCL/index.html",
    "href": "posts/review_DSATCL/index.html",
    "title": "Book Notes: Data Science at the Command Line",
    "section": "",
    "text": "Basics of building data pipelines using command line tools. On the basics the book goes into a little detail on the difference between binary executables (machine code), shell builtin’s (like ls or pwd, interpreted scripts (writing things in python or other languages), shell functions and aliases.\nAvailable free to read online here\nBook mostly focusses on interpreted scripts, shell functions and aliases, used to compose our own commandline tools and workflows.\nI’m using wsl2 to run these things, using the list of command line tools at the end of the book to find out what to install, because it’s useful to see what I need to do to get it running on other machines. But the docker image is pretty easy to set up as a speedier way to start! Honestly, it uses so many installed extras that I would recommend the docker route in hindsight.\nExplains how to pipe these together, which was handy for me because a) suddenly what R tidyverse is trying to do makes more sense and b) a lot of little commands in the scriptier or base parts of Python and R suddenly make far more sense! Eg; grepl in R from the grep builtin (is it builtin?), head…\n$ head -n 3 movies.txt\nMatrix\nStar Wars\nHome Alone\nFirst line of a script is what defines what executes it, eg for bash, R and Python respectively;\n#!/bin/bash\n#!/usr/bin/env Rscript\n#!/usr/bin/env python3"
  },
  {
    "objectID": "posts/review_DSATCL/index.html#the-book",
    "href": "posts/review_DSATCL/index.html#the-book",
    "title": "Book Notes: Data Science at the Command Line",
    "section": "",
    "text": "Basics of building data pipelines using command line tools. On the basics the book goes into a little detail on the difference between binary executables (machine code), shell builtin’s (like ls or pwd, interpreted scripts (writing things in python or other languages), shell functions and aliases.\nAvailable free to read online here\nBook mostly focusses on interpreted scripts, shell functions and aliases, used to compose our own commandline tools and workflows.\nI’m using wsl2 to run these things, using the list of command line tools at the end of the book to find out what to install, because it’s useful to see what I need to do to get it running on other machines. But the docker image is pretty easy to set up as a speedier way to start! Honestly, it uses so many installed extras that I would recommend the docker route in hindsight.\nExplains how to pipe these together, which was handy for me because a) suddenly what R tidyverse is trying to do makes more sense and b) a lot of little commands in the scriptier or base parts of Python and R suddenly make far more sense! Eg; grepl in R from the grep builtin (is it builtin?), head…\n$ head -n 3 movies.txt\nMatrix\nStar Wars\nHome Alone\nFirst line of a script is what defines what executes it, eg for bash, R and Python respectively;\n#!/bin/bash\n#!/usr/bin/env Rscript\n#!/usr/bin/env python3"
  },
  {
    "objectID": "posts/review_DSATCL/index.html#tools",
    "href": "posts/review_DSATCL/index.html#tools",
    "title": "Book Notes: Data Science at the Command Line",
    "section": "Tools",
    "text": "Tools\nThe Unix philosophy (Art of Unix Programming is a great text). Then pipe between those (good, focussed) tools. Based on management of stdin, stdout, stderr. If you really need an example of this, type rev and play around.\nA very short example, counting the number of chapters in Adventures in Wonderland (including downloading with curl)\n$ curl -s \"https://www.gutenberg.org/files/11/11-0.txt\" |\n&gt; grep \" CHAPTER\" |\n&gt; wc -l\n12\n(And use &gt; and &gt;&gt; to output to a file if you want, or read from those files with cat).\n2&gt; redirects the error messages to files, used this in Data Engineering at least once while debugging something long-running in an inadvisably hands-on manner in Azure.\nLots of the tools used are actually non-standard bits created by Jeronen or others - though that these can be created, distributed and reused so easily is kind of the point. Jeroen’s own tools - dsutls\n\nHandy bit for installing the tools I’ve been playing with:\nsudo apt install cowsay rust-bat moreutils csvkit\nSome things I’ve found super useful: csvlook/csv2md, batcat (in book as “bat” but there’s a name clash when I install on 24.04), tee for pushing out intermediate results. And of course, I always forget “man” is an option because we have the internet now… and it turns out I can press “/” to search these pages! Probably other things that open in this reader mode! jq super useful when dealing with web data/API’s/config files. Literally never heard of the tldr command…\ncsvlook -I prevents type inference and formatting eg; of numbers (interpreted years as numbers and added thousand commas without this).\n\ncsvgrep for searching/filtering csv\njq for working through json keys\n\n$ &lt; wikimedia-stream-sample sed -n 's/^data: //p' |\n&gt; jq 'select(.type == \"edit\" and .server_name == \"en.wikipedia.org\") | .title'"
  },
  {
    "objectID": "posts/review_DSATCL/index.html#making-scripts",
    "href": "posts/review_DSATCL/index.html#making-scripts",
    "title": "Book Notes: Data Science at the Command Line",
    "section": "Making scripts",
    "text": "Making scripts\nFor example, here’s a surprisingly sophisticated bash script:\nSee this page\ncurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |\n&gt; tr '[:upper:]' '[:lower:]' |\n&gt; grep -oE \"[a-z\\']{2,}\" |\n&gt; sort |\n&gt; uniq -c |\n&gt; sort -nr |\n&gt; head -n 10\nJust start writing your commands, if no input is defined in your script, will take from stdin, which means you can pipe stuff at it. Defining arguments is simple too. Names them rather than using directly for readability.\nSee example here"
  },
  {
    "objectID": "posts/review_DSATCL/index.html#documentation-and-security",
    "href": "posts/review_DSATCL/index.html#documentation-and-security",
    "title": "Book Notes: Data Science at the Command Line",
    "section": "Documentation and security",
    "text": "Documentation and security\nIt mentions docopt as a tool for adding help docs? Should look in to that. Also ShellCheck for scanning bash code for issues/vulnerabilities."
  },
  {
    "objectID": "posts/review_DSATCL/index.html#thoughts",
    "href": "posts/review_DSATCL/index.html#thoughts",
    "title": "Book Notes: Data Science at the Command Line",
    "section": "Thoughts",
    "text": "Thoughts\n\nBook is mostly useful for learning bash/shell/zsh/unix if you don’t know it already, that’s me for sure, probably not essential. Also, the command line is absolutely the kind of thing that’s so well documented that AI will be brilliant at it but who cares about that.\nI like that he’s fairly casual about not explaining every flag, command and use in the book, you need to go look things up if you want details and that’s always a good learning mechanism.\nThere are definitely tools in here I’m going to abuse - curl, weirdly, I just find more elegant than messing with python requests, and coupled with jq and csv type tools I can write something that’ll just work in a lot of places (read: VM’s in Azure) without having to mess around with venv. I also love being able to turn csv into markdown tables though I bet that exists inside Python!\nI like that he uses Nano, I know clever editors based on keyboards and nothing but keyboards exist, and I will never have time to care. Actual typing is not the least efficient part of my day.\nI notice also that for applications for which tools already exist, the python version is more verbose (albeit possibly more readable? I’m not confident on that)\nThe continuous allowance for streaming data is really interesting also, not something I’ve ever had to consider as a “research” data scientist"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Martin",
    "section": "",
    "text": "Martin is a “technologist” currently working in the public sector as a data scientist, with an interest in all things tech, AI and cyberpunk! His office time is spent applying NLP and LLM tools in the field of information retrieval, getting piles of government records and administrative data to talk back to him. There’s a side of dashboarding (RShiny), Pythonic web development (Flask) and the occasional foray into the Azure ecosystem. Currently there’s a lot of data processing and statistics (R).\nThere are also the weird project requests, building tools to figure out where the buses are, or what just happened to the jobs market.\nHis free time is spent playing co-op PC games, reading bad sci-fi, hiking and camping."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Martin Trying to Remember Things",
    "section": "",
    "text": "Book Notes: Data Science at the Command Line\n\n\n\nbook review\n\n\n\n\n\n\n\n\n\nMay 18, 2025\n\n\nMartin Wood\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: Structure and Interpretation of Computer Programs\n\n\n\nbook review\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nMartin Wood\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nMartin Wood\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/review_SICP/index.html",
    "href": "posts/review_SICP/index.html",
    "title": "Book Review: Structure and Interpretation of Computer Programs",
    "section": "",
    "text": "Oh dear lord the maths. Ok.\n\n\nMy current data science office is pretty driven, conveniently one of our number is a very experienced programmer and has offered to help guide us through reading Structure and Interpretation of Computer Programs (a helpful PDF version there). This book is reportedly a bit special, with a reputation for turning people’s brains around and change their view of programming. It’s also pretty old, dating from back when people really had to care about what they could fit into memory and how performant their algorithms were (as opposed to the Data Science approach of “Too slow eh? More cores/memory/VM’s/funding!”). The foreword begins:\n\nThis is the second edition SICP book, from Unofficial Texinfo Format. You are probably reading it in an Info hypertext browser, such as the Info mode of Emacs. You might alternatively be reading it TEX-formatted on your screen or printer, though that would be silly.\n\nIn hindsight this perfectly captured what I was in for - created by people who use Emacs and think formatted text is a sign of weakness…\nI’m one of the many people who are now kind of reversing into computer science, having used computational tools for data analysis before steadily sinking into more sophisticated programming tools as my problems became more complicated and my need to perform shadow IT around organisational limitations became more and more pressing. I suspect it’s a natural consequence of all modern companies accidentally becoming IT companies, whether they want to or not.\n\n\n\nShort version: While the actual text is very readable, the exercises were very much meant for people with a stronger background in maths. I studied a natural science, integration was considered a bit fancy. A lot of the (probably elegant) exercise questions in this book involve implementing some equation that would probably make perfect sense to me and be obviously relevant if I hadn’t spent my degree dripping acid on rocks to see if they fizzed.\nAs it is I was left staring at the question wondering if I’m supposed to care or if a “trigonometric identity” might just mean a triangle or something? Whatever. Thankfully our office’s tame wizard is well aware, and is able to help direct us to what we need to know and are meant to be drawing from sections and exercises. It’s also abundantly clear that this is a course textbook, meant to be drawn from by a teacher as needed. Knowing this is useful both in that it reassures me it’s ok to seek help and that I can occasionally skip an impenetrably mathematical exercise and lose nothing but knowledge of some weird and unusual number series. I’m ok with that.\nThe book is very much still worth working through - it leaves me knowing some more of the terminology and concepts I feel I’ve missed in my backwards slide into working with computers. A goal of mine is to steadily work through the curriculum set out by Teach Yourself Computer Science, which suggests getting through chapters 1 - 3 including exercises.\n\n\n\nHaving only just reached the end of Chapter 1, I’ve picked up lots of basics, but in more clearly defined detail than I’ve previously owned them. For example, the book has gone into detail on recursive vs iterative algorithms, and how to map from one to the other, and how they are executed by the interpreter. Terminology I didn’t know I needed or thought I understood from general knowledge has been clarified as something more specific in CS (Computer Science), like the difference between a recursive algorithm and a recursive process.\nThe insight into what interpreters think of/about the code I write, and the numerous illustrations of how execution of a more complex program folds out into a tree-like structure of executed primitive operations, are both giving me a view of how computers work that I may have seen before but is now far more explicit, and stands in attractive contrast to the many hands-on high level tutorials on applications of programming to things like data analysis that pay little attention to what is going on below the level of Pandas’ methods.\nAt a higher level, there’s also been some good discussion and examples in the book of proper abstraction; building procedures to perform specific, higher level tasks that are clear and focussed enough that once defined, you can use those without ever needing to reconsider their internal workings. Abstraction and modular code are ideas that’ll be old-hat to most people, but the\nAnd of course, now I know some Scheme! I have to admit its simplicity and lack of too much syntactic sugar (see, I know bigger words now!) make it a very nice language for demonstrating how the interpreter works through the instructions you’re feeding it. And being forced to be conscious of all those nested brackets assists your awareness of execution order.\n\n\n\nI’m using Racket, a descendant of scheme that I chose purely because I could easily download and install it on my Windows laptop, so that I could learn offline on trains (welcome to the North of England, we don’t trust in pervasive internet up here). I’ve been using that in conjunction with the PDF version of the book from MIT, because I’m old so I like that it looks like a textbook that way.\nI know others working through the book have been using online implementations of Scheme for simplicity and because our work laptops won’t trust us to install things, here’s one if you need it: try.scheme.org.\nFor myself, so far I’ve been copy-pasting code snippets over from the book and my notes like a pleb. In all of chapter 1, only once was I working on a compound procedure (big words again! Read: a small program with multiple functions) where I felt like using an IDE properly might help :P\n\n\n\nIt’s rare that my work either as a data scientist or as a data engineer has required CS knowledge - but on occassion, every few months or so, I find myself having to write something that is effectively performing a tree search to dig through weirdly stored logs. Or I need to perform matrix multiplication to create a speedier version of some geographic operation that geopandas takes hours on. On these weird issues I need to do actual programming and this book is good instruction and practice fodder for that.\nI know I’ve got a lot to learn on how to actually structure a project, at every level. A better understanding of how to abstract code, and when to do so, will be useful as I move to a few larger projects. At a fine level, learning more about the proper use of lexical scope and computational complexity will let me write clearer, simpler code that won’t hit “that won’t scale” gotcha’s."
  },
  {
    "objectID": "posts/review_SICP/index.html#what-are-you-doing",
    "href": "posts/review_SICP/index.html#what-are-you-doing",
    "title": "Book Review: Structure and Interpretation of Computer Programs",
    "section": "",
    "text": "My current data science office is pretty driven, conveniently one of our number is a very experienced programmer and has offered to help guide us through reading Structure and Interpretation of Computer Programs (a helpful PDF version there). This book is reportedly a bit special, with a reputation for turning people’s brains around and change their view of programming. It’s also pretty old, dating from back when people really had to care about what they could fit into memory and how performant their algorithms were (as opposed to the Data Science approach of “Too slow eh? More cores/memory/VM’s/funding!”). The foreword begins:\n\nThis is the second edition SICP book, from Unofficial Texinfo Format. You are probably reading it in an Info hypertext browser, such as the Info mode of Emacs. You might alternatively be reading it TEX-formatted on your screen or printer, though that would be silly.\n\nIn hindsight this perfectly captured what I was in for - created by people who use Emacs and think formatted text is a sign of weakness…\nI’m one of the many people who are now kind of reversing into computer science, having used computational tools for data analysis before steadily sinking into more sophisticated programming tools as my problems became more complicated and my need to perform shadow IT around organisational limitations became more and more pressing. I suspect it’s a natural consequence of all modern companies accidentally becoming IT companies, whether they want to or not."
  },
  {
    "objectID": "posts/review_SICP/index.html#how-is-it-going",
    "href": "posts/review_SICP/index.html#how-is-it-going",
    "title": "Book Review: Structure and Interpretation of Computer Programs",
    "section": "",
    "text": "Short version: While the actual text is very readable, the exercises were very much meant for people with a stronger background in maths. I studied a natural science, integration was considered a bit fancy. A lot of the (probably elegant) exercise questions in this book involve implementing some equation that would probably make perfect sense to me and be obviously relevant if I hadn’t spent my degree dripping acid on rocks to see if they fizzed.\nAs it is I was left staring at the question wondering if I’m supposed to care or if a “trigonometric identity” might just mean a triangle or something? Whatever. Thankfully our office’s tame wizard is well aware, and is able to help direct us to what we need to know and are meant to be drawing from sections and exercises. It’s also abundantly clear that this is a course textbook, meant to be drawn from by a teacher as needed. Knowing this is useful both in that it reassures me it’s ok to seek help and that I can occasionally skip an impenetrably mathematical exercise and lose nothing but knowledge of some weird and unusual number series. I’m ok with that.\nThe book is very much still worth working through - it leaves me knowing some more of the terminology and concepts I feel I’ve missed in my backwards slide into working with computers. A goal of mine is to steadily work through the curriculum set out by Teach Yourself Computer Science, which suggests getting through chapters 1 - 3 including exercises."
  },
  {
    "objectID": "posts/review_SICP/index.html#what-do-you-think-youre-learning",
    "href": "posts/review_SICP/index.html#what-do-you-think-youre-learning",
    "title": "Book Review: Structure and Interpretation of Computer Programs",
    "section": "",
    "text": "Having only just reached the end of Chapter 1, I’ve picked up lots of basics, but in more clearly defined detail than I’ve previously owned them. For example, the book has gone into detail on recursive vs iterative algorithms, and how to map from one to the other, and how they are executed by the interpreter. Terminology I didn’t know I needed or thought I understood from general knowledge has been clarified as something more specific in CS (Computer Science), like the difference between a recursive algorithm and a recursive process.\nThe insight into what interpreters think of/about the code I write, and the numerous illustrations of how execution of a more complex program folds out into a tree-like structure of executed primitive operations, are both giving me a view of how computers work that I may have seen before but is now far more explicit, and stands in attractive contrast to the many hands-on high level tutorials on applications of programming to things like data analysis that pay little attention to what is going on below the level of Pandas’ methods.\nAt a higher level, there’s also been some good discussion and examples in the book of proper abstraction; building procedures to perform specific, higher level tasks that are clear and focussed enough that once defined, you can use those without ever needing to reconsider their internal workings. Abstraction and modular code are ideas that’ll be old-hat to most people, but the\nAnd of course, now I know some Scheme! I have to admit its simplicity and lack of too much syntactic sugar (see, I know bigger words now!) make it a very nice language for demonstrating how the interpreter works through the instructions you’re feeding it. And being forced to be conscious of all those nested brackets assists your awareness of execution order."
  },
  {
    "objectID": "posts/review_SICP/index.html#practical-notes",
    "href": "posts/review_SICP/index.html#practical-notes",
    "title": "Book Review: Structure and Interpretation of Computer Programs",
    "section": "",
    "text": "I’m using Racket, a descendant of scheme that I chose purely because I could easily download and install it on my Windows laptop, so that I could learn offline on trains (welcome to the North of England, we don’t trust in pervasive internet up here). I’ve been using that in conjunction with the PDF version of the book from MIT, because I’m old so I like that it looks like a textbook that way.\nI know others working through the book have been using online implementations of Scheme for simplicity and because our work laptops won’t trust us to install things, here’s one if you need it: try.scheme.org.\nFor myself, so far I’ve been copy-pasting code snippets over from the book and my notes like a pleb. In all of chapter 1, only once was I working on a compound procedure (big words again! Read: a small program with multiple functions) where I felt like using an IDE properly might help :P"
  },
  {
    "objectID": "posts/review_SICP/index.html#how-might-this-be-useful",
    "href": "posts/review_SICP/index.html#how-might-this-be-useful",
    "title": "Book Review: Structure and Interpretation of Computer Programs",
    "section": "",
    "text": "It’s rare that my work either as a data scientist or as a data engineer has required CS knowledge - but on occassion, every few months or so, I find myself having to write something that is effectively performing a tree search to dig through weirdly stored logs. Or I need to perform matrix multiplication to create a speedier version of some geographic operation that geopandas takes hours on. On these weird issues I need to do actual programming and this book is good instruction and practice fodder for that.\nI know I’ve got a lot to learn on how to actually structure a project, at every level. A better understanding of how to abstract code, and when to do so, will be useful as I move to a few larger projects. At a fine level, learning more about the proper use of lexical scope and computational complexity will let me write clearer, simpler code that won’t hit “that won’t scale” gotcha’s."
  }
]